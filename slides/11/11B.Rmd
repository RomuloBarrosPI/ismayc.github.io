---
title: "MATH 141"
author: "Chester Ismay"
output:
 ioslides_presentation:
   incremental: true
   keep_md: yes
   logo: ../figs/griffin.png
   widescreen: yes
subtitle: Simple Linear Regression II
---

```{r setup, include=FALSE}
library(knitr)
options(digits=3)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
library(plotly)
poverty <- read.delim("poverty.txt", header = TRUE) %>% select(-Percent)
```


## Estimation in R {.smaller .build}

```{r fitlm}
m1 <- lm(Graduates ~ Poverty, data = poverty)
summary(m1)
```


## The `lm` object {.build .smaller}

```{r showlm}
attributes(m1)
m1$fit

```

## The `lm` object {.build .smaller}

```{r showlm2}
m1$coef
m1$residuals
```


## Interpretation of $b_1$ {.build}

The **slope** describes the estimated difference in the $y$ variable if the explanatory
variable $x$ for a case happened to be one unit larger.

```{r}
m1$coef[2]
```

*For each additional percentage point of people living below the poverty level,
we expect a state to have a proportion of high school graduates that is 0.898
lower*.

**Be Cautious**: if it is observational data, you do not have evidence of a 
*causal link*, but of an association, which still can be used for prediction.


## Interpretation of $b_0$ {.build}

The **intercept** is the estimated $y$ value that will be taken by a case with 
an $x$ value of zero.

```{r}
m1$coef[1]
```

While necessary for prediction, the intercept often has no meaningful interpretation.


# Outliers

## What is an outlier? {.build}

<div class="columns-2">
![](http://marcoghislanzoni.com/blog/wp-content/uploads/2013/10/outliers_gladwell.jpg)

**Outlier** is a general term to describe a data point that doesn't follow the
pattern set by the bulk of the data, when one takes into account the model.
</div>


## Outlier Example One

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# this chunk sets the chunk options for the whole document
require(knitr)
opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
library(openintro)
COL <- c('#55000088','#225588')
set.seed(238)
n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42, runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19, c(-17,-20,-21,-19), 12, -23.2)
i <- 1
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Two

```{r, echo=FALSE}
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Three

```{r, echo=FALSE}
i <- 3
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four

```{r, echo=FALSE}
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four (modified)

```{r, echo=FALSE}
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x[1:70], y[1:70], col = COL[2], lCol = COL[1], lwd = 3, xlim = range(x), ylim = range(y))
```

## Outliers, leverage, influence {.build}

**Outliers** are points that don't fit the trend in the rest of the data.

**High leverage points** have the potential to have an unusually large influence 
on the fitted model.

**Influential points** are high leverage points that cause a very different
line to be fit than would be with that point removed.


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
library(faraway)
data(star)
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
```


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
legend("top", inset = 0.05, "w/ outliers", lty = 2, lwd = 2, col = "darkgreen")
```


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
abline(lm(light[temp>4]~temp[temp>4], data = star), col = COL[1], lwd = 3)
legend("top", inset = 0.05, c("w/ outliers","w/o outliers"), lty = c(2,1), lwd = c(2,3), col = c("darkgreen",COL[1]))
```


## Example of high leverage, low influence

```{r, echo=FALSE}
set.seed(12)
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
y <- y - mean(y)
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## From leverage to influence

**Leverage** measures the weight given to each point in determining the regression
line.

**Influence** measures how different the regression line would be without a given
point. Often measured with *Cook's Distance*.

```{r, echo=FALSE, fig.height=4, }
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mfrow = c(1, 2))
plot(x, y, col = COL[2], pch = 16)
abline(lm(y ~ x), col = COL[1], lwd = 3)
x2 <- x[1:70]
y2 <- y[1:70]
plot(x2, y2, col = COL[2], pch = 16, xlim = range(x), ylim = range(y))
abline(lm(y2 ~ x2), col = COL[1], lwd = 3)
```


## {.smaller}

In the following plots are there outliers, leverage pts, or influential pts?

<center>
<img src="../figs/outliers.png" height = 550>
</center>


# Some chatter from the internets

## 2016 Election {.build}
<center>
<img src="../figs/538.png" width = 850>
</center>

**Question at hand**: How will Obama's 46% approval rating effect his
party's candidate for the 2016 presidential election?


## 
<center>
<img src="../figs/538-1.png" width = 850>
</center>


##  {.build}
<center>
<img src="../figs/538-2.png" width = 850>
</center>

</br>
</br>

### How would you visualize this data?

##  {.build}
<center>
<img src="../figs/538-3.png" width = 850>
</center>

</br>

### Why is it ridiculous?


## Inference for Regression {.build}
We can fit a line through any cloud of points that we please, but if we
just have a *sample* of data, any trend we detect doesn't necessarily 
demonstrate that the trend exists in the *population* at large.


## Plato's Allegory of the Cave

<div class="centered">
![](http://4.bp.blogspot.com/-rV1c4Xh4SSE/UZshhTTdFsI/AAAAAAAACQA/1VkmOaF7WFU/s1600/plato-cave.jpg)
</div>


## Statistical Inference {.build}

**Goal**: use *statistics* calculated from data to makes inferences about the 
nature of *parameters*.

In regression,

- parameters: $\beta_0$, $\beta_1$
- statistics: $b_0$, $b_1$

Classical tools of inference:

- Confidence Intervals
- Hypothesis Tests


## Unemployment and elections {.build}
```{r echo = FALSE}
library(openintro)
data(unempl)
data(house)
data(president); pres <- president
year   <- seq(1898, 2010, 4)+1
n      <- length(year)
unemp  <- rep(0, n)
change <- rep(0, n)
presid <- rep("", n)
party  <- rep("", n)
for(i in 1:n){
	urow <- which(unempl$year == year[i]-1)
	if(i < n){
		prow <- which(pres$end > year[i])[1]
	} else {
		prow <- which(pres$potus == "Barack Obama")
	}
	hrow <- which(house$yearEnd >= year[i])[1]
	party[i] <- as.character(pres$party[prow])
	if(substr(house$p1[hrow],1,5) == substr(party[i],1,5)){
		oldHouse <- house$np1[hrow] / house$seats[hrow]
	} else {
		oldHouse <- house$np2[hrow] / house$seats[hrow]
	}
	if(substr(house$p1[hrow+1],1,5) == substr(party[i],1,5)){
		newHouse <- house$np1[hrow+1] / house$seats[hrow+1]
	} else {
		newHouse <- house$np2[hrow+1] / house$seats[hrow+1]
	}
	change[i] <- (newHouse - oldHouse)/oldHouse * 100
	presid[i] <- as.character(pres$potus[prow])
	unemp[i]  <- unempl$unemp[urow]
}

unemployPres <- data.frame(year=year, potus=presid, party=party, unemp=unemp, change=change)
unemployPres[29, 3] <- "Democratic"
```

```{r echo=FALSE, fig.width=7}
levels(unemployPres$party) <- levels(unemployPres$party)[c(1, 3, 2)]
levels(unemployPres$party)[2:3] <- c("Rep", "Dem")
qplot(x = unemp, y = change, col = party, data = unemployPres)
```

<center>
**Reigning theory**: voters will punish candidates from the President's party
at the ballot box when unemployment is high.
</center>

## Unemployment and elections

```{r echo=FALSE, fig.width=7}
m1 <- lm(change ~ unemp, data = unemployPres)
plot1 <- qplot(x = unemp, y = change, col = party, data = unemployPres) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
ggplotly(plot1)
```

<center>
**Reigning theory**: voters will punish candidates from the President's party
at the ballot box when unemployment is high.
</center>

## Unemployment and elections {.build}

```{r echo = FALSE}
library(dplyr)
ump <- filter(unemployPres, unemp < 15)
m0 <- lm(change ~ unemp, data = ump)
qplot(x = unemp, y = change, col = party, data = ump) +
  geom_abline(intercept = m0$coef[1], slope = m0$coef[2])
```

<center>
Some evidence of a negative linear relationship between unemployment level
and change in party support - or is there?
</center>

## Hypothesis Test for Regression Slope {.build}

$H_0:$ There is no relationship between unemployment level and change in 
party support.

$H_O: \beta_1 = 0$

### Method
If there is no relationship, the pairing between $X$ and $Y$ is
artificial and we can randomize:

1. Create synthetic data sets under $H_0$ by shuffling $X$.
2. Compute a new regression line for each data set and store each $b_1$.
3. See where your observed $b_1$ falls in the distribution of $b_1$'s under $H_0$.


##

```{r echo = FALSE}
set.seed(764)
ump_shuffled <- ump
```

```{r}
ump_shuffled$unemp <- sample(ump_shuffled$unemp)
qplot(x = unemp, y = change, col = party, data = ump_shuffled)
```

## First $b_1$

```{r echo = FALSE}
m1 <- lm(change ~ unemp, data = ump_shuffled)
qplot(x = unemp, y = change, col = party, data = ump_shuffled) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
```


## Second $b_1$

```{r, echo = FALSE}
ump_shuffled$unemp <- sample(ump$unemp)
m1 <- lm(change ~ unemp, data = ump_shuffled)
qplot(x = unemp, y = change, col = party, data = ump) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
```


## 100 $b_1$'s

```{r echo = FALSE, cache=TRUE}
line_df <- data.frame(matrix(rep(0, 200), ncol = 2))
for (i in 1:100) {
  ump_shuffled$unemp <- sample(ump$unemp)
  m1 <- lm(change ~ unemp, data = ump_shuffled)
  line_df[i, ] <- c(m1$coef)
}

p <- qplot(x = unemp, y = change, col = party, data = ump)

for (i in 1:100) {
  p <- p + geom_abline(intercept = line_df[i, 1], slope = line_df[i, 2],
                       alpha = .3)
}

p
```


## Sampling dist. of $b_1$

```{r echo = FALSE, message = FALSE}
qplot(line_df[, 2], geom = "density") + 
  geom_vline(xintercept = m0$coef[2], col = "tomato") +
  xlab("Simulated Sample Slope Coefficients")
```


## Hypothesis Test for Regression Slope {.smaller}

```{r}
m0 <- lm(change ~ unemp, data = ump)
summary(m0)
```


## Hypothesis Test for Regression Slope {.smaller .build}

- Each line in the summary table is a hypothesis test that the parameter is zero.
- Under certain conditions, the test statistic associated with $b$'s is distributed 
like $t$ random variables with $n - p$ degrees of freedom.

$$ \frac{b - \beta}{SE} \sim t_{df = n - p}$$

```{r}
t_stat <- (-1.0010 - 0)/0.8717
pt(t_stat, df = 27 - 2) * 2
```


## Conditions for inference

1. **Linearity**: linear trend between $X$ and $Y$, check with residual plot.
2. **Independent errors**: check with residual plot for serial correlation.
3. **Normally distributed errors**: check for linearity in qq-plot.
4. **Errors with constant variance**: look for constant spread in residual plot.


##

<center>
<img src="../figs/reg_conditions.png" width = 950>
</center>