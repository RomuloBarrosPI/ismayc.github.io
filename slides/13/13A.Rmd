---
title: "MATH 141"
author: "Chester Ismay"
output: 
 ioslides_presentation:
   incremental: true
   keep_md: yes
   logo: ../figs/griffin.png
   widescreen: yes
   html_preview: false
subtitle: Logistic Regression
---

```{r setup, include=FALSE}
library(knitr)
library(rglwidget)
options(digits=3, width=100)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(rgl)
knit_hooks$set(webgl = hook_webgl)
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
library(plotly)
```

```{r getdata, echo = FALSE, message=FALSE}
nyc <- read.csv("http://andrewpbray.github.io/data/nyc.csv")
```



## Review:  The geometry of regression models {.build}

- When you have two continuous predictors $x_1$, $x_2$, then your mean function
is 

*a plane*.

- When you have two continuous predictors $x_1$, $x_2$, and a categorical 
predictor $x_3$, then your mean function represents 

*parallel planes*.

- When you add in interaction effects, the planes become 

*tilted*.

## Model 2: Food + Decor + East {.smaller}

```{r}
m2 <- lm(Price ~ Food + Decor + East, data = nyc)
summary(m2)
```


## Model 3: Food + Decor + East + Decor:East {.smaller}

```{r}
m3 <- lm(Price ~ Food + Decor + East + Decor:East, data = nyc)
summary(m3)
```


## 3D plot

```{r echo=FALSE, eval=TRUE, webgl=TRUE}
colvec <- rep("steelblue", dim(nyc)[1])
colvec[nyc$East == 1] <- "orange"
plot3d(x = nyc$Food, y = nyc$Decor, z = nyc$Price, col = colvec, 
       xlab = "Food rating", ylab = "Decor Rating", zlab = "Price")
coefs <- m3$coef
rgl.viewpoint(zoom = .7)
planes3d(coefs["Food"], coefs["Decor"], -1, coefs["(Intercept)"],
         alpha = 0.4, col = "steelblue")
planes3d(coefs["Food"], coefs["Decor"] + coefs["Decor:East"], -1, 
         coefs["(Intercept)"] + coefs["East"], alpha = 0.4, col = "orange")
```


## Comparing Models

- The `East` term was significant in model 2, suggesting that there is a 
significant relationship between location and price.
- That term became non-significant when we allowed the slope of `Decor` to vary
with location, and that difference in slopes was also non-significant.
- Notice that the slope estimate for a given variable will almost *always* change 
depending on the other variables that are in the model.



# Logistic Regression

## Building a spam filter {.build .smaller}

```{r}
library(openintro)
head(email)
# where did this data come from / how was it collected?
```


## How was the data collected? {.flexbox .vcenter .build}

<img src="../figs/reed-email.png" width="800px" />

1. Choose a single email account
2. Save each email that comes in during a given time frame
3. Create dummy variables for each text component of interest
4. Visually classify each as spam or not


## Simple Filter A {.build}

Predicting spam or not using the presence of "winner"

```{r echo = FALSE, fig.width=6, fig.height=4}
qplot(x = winner, fill = factor(spam), data = email,
       ylab = "proportion", geom = "bar")
```

If "winner" then "spam"?


## Simple Filter B {.build}

Predicting spam or not using number of characters (in thousands)

```{r echo = FALSE, fig.width=6, fig.height = 4.5}
qplot(x = num_char, col = factor(spam), data = email, geom = "density")
```


## Simple Filter B {.build}

Predicting spam or not using log number of characters (in thousands)

```{r echo = FALSE, fig.width=6, fig.height = 4}
qplot(x = log(num_char), col = factor(spam), data = email, geom = "density")
```

If `log(num_char)` < 1, then "spam"?


## Each simple filter can be thought of as a regression model. {.build .smaller}

Remember our convention for Modeling Variable Types:

- $K$: categorical variable with 2 groups
- $G$: categorical variable with 3+ groups
- $H$: continuous variable

### Filter A
$spam \sim winner; \quad K_1 \sim K_2$

### Filter B
$spam \sim log(num\_char); \quad K \sim H$


Each one by itself has poor predictive power, so how can we combine them into
a single stronger model?

## {.flexbox .vcenter .build}

<img src="../figs/good-bad.jpg" width="800px" />



#

## Logistic Regression for B

$$spam \sim log(num\_char)$$

```{r echo = FALSE, fig.width=6, fig.height = 4}
qplot(x = log(num_char), y = spam, data = email, geom = "point",
      alpha = I(.1), ylab = "spam") + stat_smooth(method = "glm", 
                                                  method.args = list(family = "binomial"),
                                                  se = FALSE)
```


## {.smaller}
```{r}
m1 <- glm(spam ~ log(num_char), data = email, family = "binomial")
summary(m1)
```


## Interpreting Logistic Regression {.build}

1. Each row of the summary output is still a hypothesis test on that parameter being 0.
2. A positive slope estimate indicates that there is a positive association between that particular
explanatory variable and the response.
3. Each estimate is still conditional on the other variables held constant.

## Transforming from the logit (Simple case)

The logistic model approximates the $ln(odds)$ using a linear predictor $\beta_0 + \beta_1 X$. If we know
$ln(P/(1-P)) = \beta_0 + \beta_1 X$, what is the formula for $P$?

1. To go from $ln (odds)$ to $odds$, we use the exponential function $e^x$: $odds = e^{ln(odds)}$

2. You can check that if $odds = P / (1 - P)$, then solving for $P$ gives $P = odds/ (1 + odds)$.

- Putting 1 and 2 together gives \[ P = \dfrac{e^{log(odds)}}{1 + e^{log(odds)}}. \]

<center>
- The function $f(x) = e^x / (1 + e^x)$ is commonly known as the _logistic function_, thus, giving the name to logistic regression.
</center>

## A more sophisticated model {.build .smaller}

```{r}
m2 <- glm(spam ~ log(num_char) + to_multiple + attach + dollar + inherit + 
            viagra, data = email, family = "binomial")
summary(m2)
```


## Extending the model {.build}

A GLM consists of three things:

1. A linear combination of predictor variables
2. A distribution of the response variable
3. A link function between the two

- MLR : Normal distribution, identity link function

- Logistic Regression : Binomial distribution, logit link function

- Poisson Regression : Poisson distribution, logarithm link function


# Bayesian Inference


## When will this lecture end? {.flexbox .vcenter .build}

```{r echo = FALSE}
qplot(c(-15, 15), stat = "function", fun = dnorm, geom = "line",
      args = list(sd = 3.4), xlab = "minutes from the end", lwd = I(2),
      col = I("steelblue"), ylab = "prob. density")
```


##

Please draw your own subjective distributions for the following events.

1. The probability that Hillary Clinton will win the presidency.
2. The probability that, on a given night, the sun has gone super nova.
3. The total number of individual socks that you own.


## Karl Broman's Socks {.flexbox .vcenter .build}

```{r echo = FALSE, eval = FALSE}
# credit for this example goes to Rasmus Baathe, who made a similar presentation
# at userR! 2015. most of the included images are his.
```

<img src="../figs/broman-tweet.png" width="400px" />


## Classical H test {.build}

### Assert a model
$H_0$: I have $N_{pairs}$ pairs of socks and $N_{singles}$ singletons. The first 11 socks that 
I pull out of the machine are a random sample from this population.

### Decide on a test statistic
The number of singletons in the sample: 11.

### Construct the sampling distribution
Probability theory or simulation.

### See where your observed stat lies in that distribution
Find the p-value if you like.


## $H_0$ {.flexbox .vcenter .build}

<img src="../figs/pairs-socks.png" height="400px" />

$$N_{pairs} = 9$$


## $H_0$ {.flexbox .vcenter .build}

<img src="../figs/all-socks.png" height="400px" />

$$N_{pairs} = 9; \quad N_{singles} = 5$$


## Contructing the sampling dist. {.build}

We'll use simulation.

Create the population of socks:

```{r}
sock_pairs <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K")
sock_singles <- c("l", "m", "n", "o", "p")
socks <- c(rep(sock_pairs, each = 2), sock_singles)
socks
```


## One draw from the machine {.build}

```{r}
picked_socks <- sample(socks, size = 11, replace = FALSE)
picked_socks
sock_counts <- table(picked_socks)
sock_counts
n_singles <- sum(sock_counts == 1)
n_singles
```


## Our simulator {.flexbox .vcenter .build}

<img src="../figs/washing-machine.png" height="400px" />


## Constructing the sampling dist. {.build}

```{r echo = FALSE}
pick_socks <- function(N_pairs, N_singles, N_pick) {
  N_sock_types <- N_pairs + N_singles
  socks <- rep(1:N_sock_types, rep( 2:1, c(N_pairs, N_singles) ))
  picked_socks <- sample(socks, 11)
  sock_counts <- table(picked_socks)
  n_singles <- sum(sock_counts == 1)
  n_singles
}
set.seed(200)
```

```{r}
pick_socks(N_pairs = 9, N_singles = 5, N_pick = 11)
pick_socks(9, 5, 11)
pick_socks(9, 5, 11)
```

Repeat many, many times...


## The sampling distribution {.flexbox .vcenter}

```{r echo = FALSE, cache = TRUE}
sim_singles <- rep(0, 1000)

for (i in 1:1000) {
  sim_singles[i] <- pick_socks(9, 5, 11)
}

qplot(as.factor(sim_singles), geom = "bar", xlab = "number of singletons")
```


## The sampling distribution {.flexbox .vcenter}

```{r echo = FALSE}
qplot(as.factor(sim_singles), geom = "bar", xlab = "number of singletons") +
  geom_vline(xintercept = 6, col = "tomato")
```


## The p-value {.build}

Quantifying how far into the tails our observed count was.

```{r}
table(sim_singles)
table(sim_singles)[6]/1000
```

```{r echo = FALSE}
pval <- table(sim_singles)[6]/1000
```

Our two-tailed p-value is `r pval*2`.


## Question

What is the best definition for our p-value in probability notation?

1. P($H_0$ is true | data) = `r pval`
2. P($H_0$ is false | data) = `r pval`
3. P($H_0$ is true) = `r pval`
4. P(data | $H_0$ is true) = `r pval`
5. P(data) = `r pval`


## Question

What is the best definition for our p-value in probability notation?

1. P($H_0$ is true | data) = `r pval`
2. P($H_0$ is false | data) = `r pval`
3. P($H_0$ is true) = `r pval`
4. **P(data | $H_0$ is true) = `r pval`**
5. P(data) = `r pval`


## The challenges with the classical method {.build}

The result of a hypothesis test is a probability of the form:

$$ P(\textrm{ data or more extreme } | \ H_0 \textrm{ true }) $$

while most people *think* they're getting

$$ P(\ H_0 \textrm{ true } | \textrm{ data }) $$

How can we go from the former to the latter?


## What we have {.flexbox .vcenter}
<img src="../figs/classical-socks.png" width="800px" />


## What we want {.flexbox .vcenter}
<img src="../figs/bayes-socks.png" width="800px" />


# Bayesian Modeling
## Bayes Rule {.build}

$$P(A \ | \ B) = \frac{P(A \textrm{ and } B)}{P(B)} $$

$$P(A \ | \ B) = \frac{P(B \ | \ A) \ P(A)}{P(B)} $$

$$P(model \ | \ data) = \frac{P(data \ | \ model) \ P(model)}{P(data)} $$

What does it mean to think about $P(model)$?


##

Please draw your own subjective distributions for the following events.

1. The probability that Hilary will win the presidency.
2. The probability that, on a given night, the sun has gone super nova.
3. The total number of individual socks that you own.


## Prior distribution {.build .flexbox .vcenter}

A *prior distribution* is a probability distribution for a *parameter* that 
summarizes the information that you have before seeing the data.

```{r, cache = TRUE, echo = FALSE}
x <- rnbinom(1e6, mu = 30, size = -30^2 / (30 - 15^2))
(prior_n <- qplot(x, geom = "histogram", xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
      fill = I("green"), ylab = "prob. density", main = "P(parameter)"))
```


## Prior on proportion pairs {.flexbox .vcenter .build}

```{r cache = TRUE, echo = FALSE}
y <- rbeta(1e6, shape1 = 15, shape2 = 2)
(prior_p <- qplot(y, geom = "histogram", xlab = "proportion of pairs", binwidth = .01, xlim = c(0, 1),
      fill = I("green"), ylab = "prob. density", main = "P(parameter)"))
```


## {.flexbox .vcenter}
<img src="../figs/abc1.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc2.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc3.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc4.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc5.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc6.png" height="550px" />


## {.flexbox .vcenter}
<img src="../figs/abc7.png" height="550px" />


## Full simulation {.build}

```{r cache = TRUE, echo = FALSE}
sock_sim <- t(replicate(100000, {
  n_socks <- rnbinom(1, mu = 30, size = -30^2 / (30 - 15^2) )
  prop_pairs <- rbeta(1, shape1 = 15, shape2 = 2)
  n_pairs <- round(floor(n_socks / 2) * prop_pairs)
  n_odd <- n_socks - n_pairs * 2
  n_sock_types <- n_pairs + n_odd
  socks <- rep(seq_len(n_sock_types), rep( 2:1, c(n_pairs, n_odd) ))
  picked_socks <- sample(socks, size =  min(11, n_socks))
  sock_counts <- table(picked_socks)
  c(unique = sum(sock_counts == 1), pairs = sum(sock_counts == 2),
    n_socks = n_socks, prop_pairs = prop_pairs)
}))
sock_sim <- as.data.frame(sock_sim)
post_samples <- sock_sim %>%
  filter(unique == 11, pairs == 0)
```

```{r}
head(sock_sim)
sock_sim %>%
  filter(unique == 11, pairs == 0) %>%
  head()
```


## Proportion of pairs

```{r echo = FALSE}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

```{r cache = TRUE, echo = FALSE, fig.width = 8.5}
post_p <- qplot(prop_pairs, data = post_samples, geom = "histogram", 
                xlab = "proportion of pairs", binwidth = .01, xlim = c(0, 1),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)")
multiplot(prior_p, post_p, layout = matrix(1:2, ncol = 2, byrow = TRUE))
```


## Number of socks

```{r cache = TRUE, echo = FALSE, fig.width = 8.5}
post_n <- qplot(n_socks, data = post_samples, geom = "histogram", 
                xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)")
multiplot(prior_n, post_n, layout = matrix(1:2, ncol = 2, byrow = TRUE))
```


## Karl Broman's Socks {.flexbox .vcenter .build}

<img src="../figs/broman-tweet.png" width="400px" />


## The posterior distribution {.build}

```{r cache = TRUE, echo = FALSE, fig.height = 3, fig.width = 5}
post_n
```

- Distribution of a parameter after conditioning on the data
- Synthesis of prior knowledge and observations (data)

### Question
What is your best guess for the number of socks that Karl has?


## Our best guess

```{r cache = TRUE, echo = FALSE, fig.height = 3, fig.width = 5}
qplot(n_socks, data = post_samples, geom = "histogram", 
                xlab = "number of socks", binwidth = 1, xlim = c(0, 100),
                fill = I("green"), ylab = "prob. density", main = "P(parameter|data)") +
  geom_vline(xintercept = median(post_samples$n_socks), col = "goldenrod")
```

- The posterior median is 44 socks.


## Karl Broman's Socks {.flexbox .vcenter .build}

<img src="../figs/broman-tweet2.png" width="600px" />

$$ 21 \times 2 + 3 = 45 \textrm{ socks} $$


## Summary {.build}

Bayesian methods . . .

- Require the subjective specification of your prior knowledge
- Provide a posterior distribution on the parameters
- Have strong intuition
- Are computationally expensive


##  {.flexbox .vcenter .build}

<img src="../figs/supernova.png" height="550px" />


